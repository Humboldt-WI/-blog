<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RESEARCH on Institute of Infomation Systems at HU-Berlin</title>
    <link>https://humboldt-wi.github.io/blog/research/</link>
    <description>Recent content in RESEARCH on Institute of Infomation Systems at HU-Berlin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://humboldt-wi.github.io/blog/research/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>BERT</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/bert_blog_post/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/bert_blog_post/</guid>
      <description>Anti Social Online Behaviour Detection with BERT Comparing Bidirectional Encoder Representations from Transformers (BERT) with DistilBERT and Bidirectional Gated Recurrent Unit (BGRU) R. Evtimov - evtimovr@hu-berlin.de
M. Falli - fallimar@hu-berlin.de
A. Maiwald - maiwalam@hu-berlin.de
Introduction Motivation In 2018, a research paper by Devlin et, al. titled “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” took the machine learning world by storm. Pre-trained on massive amounts of text, BERT, or Bidirectional Encoder Representations from Transformers, presented a new type of natural language model.</description>
    </item>
    
    <item>
      <title>Economic Uncertainty Identification</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/uncertainty_identification_transformers/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/uncertainty_identification_transformers/</guid>
      <description>Economic Uncertainty Identification Using Transformers - Improving Current Methods Authors: Siddharth Godbole, Karolina Grubinska &amp;amp; Olivia Kelnreiter   Table of Contents  Introduction Motivation and Literature Theoretical Background
3.1 Transformers, BERT and BERT-based Models
3.2 Transformers Architecture
3.3 BERT
3.4 RoBERTa
3.5 DistilBERT
3.6 ALBERT
3.7 Comparing RoBERTa, DistilBERT and ALBERT Application to Economic Policy Uncertainty
4.1 Data Exploration
4.1.1 Data Pre-Processing
4.1.2 Data Imbalance
4.2 Models Implementation
4.2.1 Transformer-Based Models</description>
    </item>
    
    <item>
      <title>Generating Synthetic Comments to Balance Data for Text Classification</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/text_generation/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/text_generation/</guid>
      <description>LatexIT.add(&#39;p&#39;,true);   Table of Contents  Introduction Data Exploration Data Pre-Processing Text Generation(1): Language Model - GloVe  Further Text Preparation Modeling Generation   Text Generation(2): Language Model - GPT-2  Why GPT-2? What makes GPT-2 so powerful? The Problem of Long-Term Dependencies GPT-2 Architecture How does GPT-2 create text? Input Encoding 1. Token Embeddings (wte) 2. Positional Encoding (wpe) GPT-2 - Token Processing Overview Self-Attention Process 1.</description>
    </item>
    
    <item>
      <title>SHOPPER: A Probabalistic Consumer Choice Model</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/group3_shopper/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/group3_shopper/</guid>
      <description>BLOG_POST   /*! * * Twitter Bootstrap * */ /*! * Bootstrap v3.3.7 (http://getbootstrap.com) * Copyright 2011-2016 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE) */ /*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */ html { font-family: sans-serif; -ms-text-size-adjust: 100%; -webkit-text-size-adjust: 100%; } body { margin: 0; } article, aside, details, figcaption, figure, footer, header, hgroup, main, menu, nav, section, summary { display: block; } audio, canvas, progress, video { display: inline-block; vertical-align: baseline; } audio:not([controls]) { display: none; height: 0; } [hidden], template { display: none; } a { background-color: transparent; } a:active, a:hover { outline: 0; } abbr[title] { border-bottom: 1px dotted; } b, strong { font-weight: bold; } dfn { font-style: italic; } h1 { font-size: 2em; margin: 0.</description>
    </item>
    
    <item>
      <title>Big Peer Review Challenge</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/group11_peer_reviews/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/group11_peer_reviews/</guid>
      <description>Big Peer Review Challenge Asena Ciloglu &amp;amp; Melike Merdan Abstract This blog post studies the first public dataset of scientific peer reviews available for research purposes PeerRead applying state-of-the-art NLP models ELMo and ULMFit to a text classification task [1]. It aims to examine the importance of the peer reviews on paper’s acceptance or rejection decision in well-known conferences of computational linguistics, AI and NLP.
Table of Contents   Introduction  Peer Review Process Motivation    Descriptive Analytics  A Dataset of Peer Reviews  Approach and Data Extraction   Google Scholarly Data Cleaning Data &amp;amp; Insights    Application  Our Approach  Content-based Classification Review-based Classification Analysis with Auxiliary Data   Transfer Learning and Recent Applications Embedding for Language Models (ELMo)  Methodology  Deep Contextualized Word Representations Model Architecture     Universal Language Model Fine Tuning (ULMFit)  Methodology  General Knowledge Domain Training Target Task Language Model Fine Tuning Target Task Classifier     Support Vector Machine (SVM)    Empirical Results &amp;amp; Conclusion  Results Discussion and Conclusion    Reference List  1.</description>
    </item>
    
    <item>
      <title>Deep Learning for Survival Analysis</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/group2_survivalanalysis/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/group2_survivalanalysis/</guid>
      <description>Deep Learning for Survival Analysis Authors: Laura Löschmann, Daria Smorodina  Table of content  Motivation - Business case Introduction to Survival Analysis  2.1 Common terms 2.2 Survival function 2.3 Hazard function   Dataset Standard methods in Survival Analysis  4.1 Kaplan - Meier estimator 4.2 Cox proportional hazards model 4.3 Time-varying Cox regression 4.4 Random survival forests   Deep Learning for Survival Analysis  5.1 DeepSurv 5.</description>
    </item>
    
    <item>
      <title>State Of The Art Text Summarisation Techniques</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text_summarization_techniques/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/nlp_text_summarization_techniques/</guid>
      <description>table.display{ margin-bottom: 25px; margin-top: 25px; } th.display,td.display { border-bottom: 1px solid #ddd; padding: 5px; text-align: left; } tr.display:hover {background-color: #f5f5f5;} .gist { width: 100%; overflow: auto; } .gist .blob-wrapper.data { max-height: 350px; overflow: auto; } * { box-sizing: border-box; } /* Create two equal columns that floats next to each other */ .column { float: left; width: 50%; padding: 10px; } /* Clear floats after the columns */ .row:after { content: &#34;</description>
    </item>
    
    <item>
      <title>Causal Neural Networks</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/group5_causal_neural_networks/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/group5_causal_neural_networks/</guid>
      <description>Causal Neural Networks - Optimizing Marketing Spendings Effectivness Hasan Reda Alchahwan, Lukas	Baumann, Darius Schulz
Table of Contents:  Introduction Literature Review Descriptive Analysis of the Dataset Estimation of Treatment Effects considering the checkout amount Estimation of Treatment Effects considering conversion Placebo Experiment Conclusion  1. Introduction Targeting the right customers in marketing campaigns has always been a struggle for marketeers. Data-driven approaches allowed to select targets with the highest probability to buy or the greatest revenue expected, if the costs of the activity deter you from targeting every customer.</description>
    </item>
    
    <item>
      <title>Data Generating Process Simulation: The opossum package</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/data_generating_process_blogpost/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/data_generating_process_blogpost/</guid>
      <description>Data Generating Process Simulation&amp;#182;    1&amp;nbsp;&amp;nbsp;Introduction1.1&amp;nbsp;&amp;nbsp;Topic1.2&amp;nbsp;&amp;nbsp;Motivation1.3&amp;nbsp;&amp;nbsp;Properties2&amp;nbsp;&amp;nbsp;Review2.1&amp;nbsp;&amp;nbsp;Literature2.2&amp;nbsp;&amp;nbsp;Software3&amp;nbsp;&amp;nbsp;Theory behind the package3.1&amp;nbsp;&amp;nbsp;General Model: Partial Linear Regression3.2&amp;nbsp;&amp;nbsp;Generating Covariates3.2.1&amp;nbsp;&amp;nbsp;Continuous Covariates3.2.2&amp;nbsp;&amp;nbsp;Binary and categorical covariates3.3&amp;nbsp;&amp;nbsp;Treatment assignment3.3.1&amp;nbsp;&amp;nbsp;Random3.3.2&amp;nbsp;&amp;nbsp;Dependent on covariates3.4&amp;nbsp;&amp;nbsp;Treatment effects3.4.1&amp;nbsp;&amp;nbsp;Positive &amp;amp; negative constant effect3.4.2&amp;nbsp;&amp;nbsp;Positive &amp;amp; negative continuous heterogeneous effect3.4.3&amp;nbsp;&amp;nbsp;No effect3.4.4&amp;nbsp;&amp;nbsp;Discrete heterogeneous treatment effect3.5&amp;nbsp;&amp;nbsp;Output variable3.5.1&amp;nbsp;&amp;nbsp;Continuous3.5.2&amp;nbsp;&amp;nbsp;Binary4&amp;nbsp;&amp;nbsp;Package application4.1&amp;nbsp;&amp;nbsp;Choosing covariates4.2&amp;nbsp;&amp;nbsp;Creating treatment effects4.3&amp;nbsp;&amp;nbsp;Creating output4.4&amp;nbsp;&amp;nbsp;Other functions5&amp;nbsp;&amp;nbsp;Example: Applying double machine learning6&amp;nbsp;&amp;nbsp;Discussion7&amp;nbsp;&amp;nbsp;References    Introduction&amp;#182;Topic&amp;#182;As modern science becomes increasingly data-driven among virtually all fields, it is obligatory to inspect not only how scientists analyze data but also what kind of data is used.</description>
    </item>
    
    <item>
      <title>Efficient Experiments Through Inverse Propensity Score Weighting</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/efficient_a_b_testing_propensity_scoring/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/efficient_a_b_testing_propensity_scoring/</guid>
      <description>APA_Blogpost_latest   /*! * * Twitter Bootstrap * */ /*! * Bootstrap v3.3.7 (http://getbootstrap.com) * Copyright 2011-2016 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE) */ /*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */ html { font-family: sans-serif; -ms-text-size-adjust: 100%; -webkit-text-size-adjust: 100%; } body { margin: 0; } article, aside, details, figcaption, figure, footer, header, hgroup, main, menu, nav, section, summary { display: block; } audio, canvas, progress, video { display: inline-block; vertical-align: baseline; } audio:not([controls]) { display: none; height: 0; } [hidden], template { display: none; } a { background-color: transparent; } a:active, a:hover { outline: 0; } abbr[title] { border-bottom: 1px dotted; } b, strong { font-weight: bold; } dfn { font-style: italic; } h1 { font-size: 2em; margin: 0.</description>
    </item>
    
    <item>
      <title>Matching Methods for Causal Inference: A Machine Learning Update</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/matching_methods/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/matching_methods/</guid>
      <description>Authors: Samantha Sizemore and Raiber Alkurdi Introduction Practitioners from quantitative Social Sciences such as Economics, Sociology, Political Science, Epidemiology and Public Health have undoubtedly come across matching as a go-to technique for preprocessing observational data before treatment effect estimation; those on the machine learning side of the aisle, however, may be unfamiliar with the concept of matching. This blog post aims to provide a succinct primer for matching neophytes and, for those already familiar with this technique, an overview of how state-of-the-art machine learning can be incorporated into the matching process.</description>
    </item>
    
    <item>
      <title>Uplift Modelling with Multiple Treatments</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/multiple_treatments_uplift/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/multiple_treatments_uplift/</guid>
      <description>Applications of Causal Inference for Marketing: Estimating Treatment Effects for multiple Treatments Authors: Jan Krol and Matthias Becher Table of Contents  Introduction
 Common Marketing Challenges
 Models  3.1 Decision Trees Rzepakowski &amp;amp; Jaroszewicz  3.1.1 Basic Rzepakowski &amp;amp; Jaroszewicz  3.1.2 Simple Splitting Criterion  3.2 Causal Tree and Causal Forest  3.3 Separate Model  Evaluation Methods
4.1 Uplift Curves 4.2 Expected Outcome Experimental Setup  Results Outlook References  1.</description>
    </item>
    
    <item>
      <title>Impact of Microfinance on Social Well-being</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/microfinance-policy/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/microfinance-policy/</guid>
      <description>Microfinance Policies  Impact of Microfinance on The Social Well-being Authors: Edanur Kahvecioglu and Yu-Tang Wu Abstract Is microcredit a miracle or just a hype? While more and more researches were conducted to study microcredits, people started to question the effectiveness of microcredit program. This project explores the heterogeneity of treatment effect on microcredit to households&amp;rsquo; well-being. Causuall Random Forest and Two-model approach are used to analyze treatment effects on the household level and identify the important variables that separate households with higher treatment effect.</description>
    </item>
    
    <item>
      <title>Social Pressure and Voter Turnout - A Causal Machine Learning Approach</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/social_pressure/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/social_pressure/</guid>
      <description>Revisiting Social Pressure and Voter Turnout: Causal Inference with Supervised Learning Methods Julius Reimer &amp;amp; Toby Chelton
[Source: https://paulkiser.com/2016/02/04/five-fixes-for-our-primarycaucus-fiasco/] Table of contents 1. Introduction
2. Original Study and Our Objectives
3. Data and Random Assignment
4. Modelling
5. Results
6. Conclusion
7. References
1. Introduction The issue of voter turnout has long been examined in both academic and media spheres. In fact, the interest dates back many years to papers such as Gosnell’s 1927 ‘Getting out the vote- an experiment in the stimulation of voting’.</description>
    </item>
    
    <item>
      <title>Marketing Campaign Optimization: Profit modeling</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/01marketing_campaign_optimization/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/01marketing_campaign_optimization/</guid>
      <description>Marketing Campaign Optimization ## Causal Inference in Profit Uplift Modeling #### Authors: Asmir Muminovic, Lukas Kolbe ### Motivation The global spending on advertising amounts to more than 540 billion US dollars for 2018 only, and the spending for 2019 is predicted to reach over 560 billion US dollars. The marketing spendings have continuously increased since 2010 [17], and this trend does not seem to brittle in the near future. Although marketing is such an integral part of most businesses, marketers often fail to maximize the profitability of their marketing efforts.</description>
    </item>
    
    <item>
      <title>Causal KNN</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/blog_post_causal_knn/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/blog_post_causal_knn/</guid>
      <description>LatexIT.add(&#39;p&#39;,true);   Applied Predictive Analytics Seminar - Causal KNN Beyond estimating the overall effect of a treatment, the uplift, econometric and statistical literature have set their eyes on estimating the personal treatment effect for each individual. This blogpost highly relates to the paper of Hitsch &amp;amp; Misra (2018), where a novel, direct uplift modeling approach is introduced, called Causal KNN. The k-nearest neighbour algorithm provides an interesting opportunity for the estimation of treatment effects in small groups.</description>
    </item>
    
    <item>
      <title>Correcting for Self-selection in Product Rating: Causal Recommender Systems</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/causalrecommendersystem/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/causalrecommendersystem/</guid>
      <description>Correcting for Self-selection in Product Rating: Causal Recommender Systems Authors: Karolina Grubinska &amp;amp; Médéric Thomas Table of Contents  An Introduction to Recommender Systems [Motivation] (#motivation) [Overview of Methods] (#methods)   [Matrix Completion Problem] (#matrix) [Singular Value Decomposition] (#svd)  [Problem Definition] (#problem)   [The Missing-at-Random Assumption] (#mar) [Does MAR really hold?] (#marhold)  [Our Goal in This Framework] (#goal)   [In Mathematical Terms] (#maths)  [A proposal for Causal Recommender System: Causal Embeddings] (#cause)   [Causal Setup] (#causal) [Results Comparison] (#results)  [Conclusions] (#conclusions) [Bibliography] (#bibliography)  An Introduction to Recommender Systems  One of the main characteristics of a modern, digital society that we currently live in is the heterogeneity of available items.</description>
    </item>
    
    <item>
      <title>Implementation of the Double/ Debiased Machine Learning Approach in Python</title>
      <link>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/double_machine_learning/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/applied_predictive_modeling_19/double_machine_learning/</guid>
      <description>Double Machine Learning Implementation  
Christopher Ketzler*, Guillermo Morishige*
  Abstract:	The aim of this paper is to replicate and apply the approach provided by Chernozhukov et al. (2016) to get the causal estimand of interest: average treatment effect (ATE) $\ \eta_0 $ using Neyman orthogonality and cross-fitting. For observational data, we will estimate the causal relationship between the eligibility and participation in the 401(k) and its effect on net financial assets; as well to apply it to other datasets, to find the effect of the Pennsylvania Reemployment Bonus on the unemployment duration and the effect of smoking on medical costs.</description>
    </item>
    
    <item>
      <title>Uncertainty in Profit Scoring (Bayesian Deep Learning)</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/uncertainty-and-credit-scoring/</link>
      <pubDate>Sat, 09 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/uncertainty-and-credit-scoring/</guid>
      <description>Uncertainty in Profit Scoring (Bayesian Deep Learning) Djordje Dotlic, Batuhan Ipekci, Julia Dullin Contents  Introduction Literature Review Theory
A. Bayesian Inference
B. Variational Inference
C. Monte Carlo Dropout
 Data Exploration
 Results and Evaluation
  Introduction  The problem of credit scoring is a very standard one in Machine Learning literature and applications. Predicting whether or not a loan applicant will go default is one of the typical examples of classification problem, and usually serves as a good ground for application and comparison of various machine learning techniques- which, over the years, became very precise in making a binary prediction.</description>
    </item>
    
    <item>
      <title>Building a LDA-based Book Recommender System</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/is_lda_final/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/is_lda_final/</guid>
      <description>IS_LDA_final  /*! * * Twitter Bootstrap * */ /*! * Bootstrap v3.3.7 (http://getbootstrap.com) * Copyright 2011-2016 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE) */ /*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */ html { font-family: sans-serif; -ms-text-size-adjust: 100%; -webkit-text-size-adjust: 100%; } body { margin: 0; } article, aside, details, figcaption, figure, footer, header, hgroup, main, menu, nav, section, summary { display: block; } audio, canvas, progress, video { display: inline-block; vertical-align: baseline; } audio:not([controls]) { display: none; height: 0; } [hidden], template { display: none; } a { background-color: transparent; } a:active, a:hover { outline: 0; } abbr[title] { border-bottom: 1px dotted; } b, strong { font-weight: bold; } dfn { font-style: italic; } h1 { font-size: 2em; margin: 0.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks - sales forecast</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/cnn_sales_forecast/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/cnn_sales_forecast/</guid>
      <description>Authors: Jakub Kondek, Karim Chennoufi, Kevin Noessler Introduction Motivation The increasing popularity of websites such as Instagram, Facebook or Youtube has lead to an increase in visual data over the last few years. These websites have become part of everyday life for many people and are therefore used excessively. The use of such websites has contributed among other things to enormously increasing amount of visual data in the process. Every day thousands of images and videos are uploaded, making it virtually impossible to analyze them by hand, as the sheer mass of content does not allow it.</description>
    </item>
    
    <item>
      <title>Generative Models</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/generativemodels/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/generativemodels/</guid>
      <description>Authors: Gabriel Blumenstock, Yu Fan, Yang Tian Introduction What are generative models? In machine learning, generative models are used to generate new samples following the same distribution of the original data using unsupervised learning algorithms. Such methods provide a powerful way to detect and analyze enormous information of data, which has been applied to various domains, e.g. images and texts. By learning the statistical latent space of images or stories, the models are able to obtain human experiences and then “create” similar meaningful outputs.</description>
    </item>
    
    <item>
      <title>Opening the black box of machine learning</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/openingtheblackbox/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/openingtheblackbox/</guid>
      <description>Authors: Christopher Ketzler, Stefan Grosse, Raiber Alkurdi Motivation The development of machine learning or deep learning (ML/DL) models which provides the user to have a decision to a specific problem has progressed enormously in the last years. The emergence of better computer hardware and therefore software, and the collecting of big data leads to more and more complicated algorithms. If these algorithms are no more understandable by average humans in terms of what are they doing or why they give a certain decision, we call them black-boxes.</description>
    </item>
    
    <item>
      <title>Text Classification with Hierarchical Attention Network</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/</guid>
      <description>Text Classification with Hierarchical Attention Networks How to assign documents to classes or topics Authors: Maria Kränkel, Hee-Eun Lee - Seminar Information System 18/19  After reading this blog post, you will know:
 What text classification is and what it is used for What hierarchical attention networks are and how their architecture looks like How to classify documents by implementing a hierarchical attention network  Introduction Imagine you work for a company that sells cameras and you would like to find out what customers think about the latest release.</description>
    </item>
    
    <item>
      <title>Crime and Neural Nets</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/02lstmgruandbeyond/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/02lstmgruandbeyond/</guid>
      <description>Crime and Neural Nets&amp;#182;Introducing Recurrent Neural Networks with Long-Short-Term Memory and Gated Recurrent Unit to predict reported Crime Incidents&amp;#182;Carolin Kunze, Marc Scheu, Thomas Siskos&amp;#182;Several police departments across the Unites States have been experimenting with software for crime prdiction. This started a controversial debate: Critics are questioning the predictiv power of the underlying machine learning models and point out biases towards certain crime typs and neighborhoods. We took this as occacion to look into the publicly available crime records of the city of chicago.</description>
    </item>
    
    <item>
      <title>ULMFiT: State-of-the-Art in Text Analysis</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/</guid>
      <description>Universal Language Model Fine-Tuning (ULMFiT) State-of-the-Art in Text Analysis  Authors: Sandra Faltl, Michael Schimpke &amp;amp; Constantin Hackober 
Table of Contents   Introduction  Literature Review and Motivation Inductive Transfer Learning Our Datasets Overview ULMFiT    General-Domain Language Model Pretraining  Word Embeddings Example of a Forward Pass through the LM Preparations for Fine-Tuning  Matching Process for the Embedding Matrix Variable Length Backpropagation Sequences Adam Optimizer Dropout      Target Task Language Model Fine-Tuning  Freezing Learning Rate Schedule Discriminative Fine-Tuning    Target Task Classifier  Concat Pooling Linear Decoder Gradual Unfreezing Benchmarks Example of a Forward Pass through the Classifier    Our Model Extension  Results Without Vocabulary Reduction    Conclusion  Reference List  1.</description>
    </item>
    
    <item>
      <title>Uplift Modelling</title>
      <link>https://humboldt-wi.github.io/blog/research/theses/uplift_modeling_blogpost/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/theses/uplift_modeling_blogpost/</guid>
      <description>Uplift Modeling Blogcode{white-space: pre;}pre:not([class]) {background-color: white;}if (window.hljs) {hljs.configure({languages: []});hljs.initHighlightingOnLoad();if (document.readyState &amp;&amp; document.readyState === &#34;complete&#34;) {window.setTimeout(function() { hljs.initHighlighting(); }, 0);}}h1 {font-size: 34px;}h1.title {font-size: 38px;}h2 {font-size: 30px;}h3 {font-size: 24px;}h4 {font-size: 18px;}h5 {font-size: 16px;}h6 {font-size: 12px;}.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks (CNN)</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/02convolutionalneuralnetworks/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/02convolutionalneuralnetworks/</guid>
      <description>Convolutional Neural Networks Authors: Elias Baumann, Franko Maximilian Hölzig, Josef Lorenz Rumberger Table of Content  Motivation Images Artificial Neural Networks Convolutional Neural Networks Architecture Overview Layers 7. Convolutional Layer 8. Filters 9. Convolution for Functions 10. Mathematical 2D convolution 11. Back to 2D convolution 12. Stride 13. Padding 14. Local receptive fields 15. Parameter Sharing 16. Weight Initialization 17. im2col 18. Convolution using im2col 19. Activation Function Layer 20. Why is a non-linear function needed?</description>
    </item>
    
    <item>
      <title>Financial Time Series Predicting with Long Short-Term Memory</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/06financialtime-series/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/06financialtime-series/</guid>
      <description>Financial Time Series Predicting with Long Short-Term Memory Authors: Daniel Binsfeld, David Alexander Fradin, Malte Leuschner Introduction Failing to forecast the weather can get us wet in the rain, failing to predict stock prices can cause a loss of money and so can an incorrect prediction of a patient’s medical condition lead to health impairments or to decease. However, relying on multiple information sources, using powerful machines and complex algorithms brought us to a point where the prediction error is as little as it has ever been before.</description>
    </item>
    
    <item>
      <title>Image Analysis: Introduction to deep learning for computer vision</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/03imageanalysis/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/03imageanalysis/</guid>
      <description>Image Analysis: Introduction to deep learning for computer vision Authors: Nargiz Bakhshaliyeva, Robert Kittel In this blog, we present the practical use of deep learning in computer vision. You will see how Convolutional Neural Networks are being applied to process the visual data, generating some valuable knowledge. In particular, we focused on the object recognition task, aiming to classify what kind of an object (a dog or a cat) is presented within a particular image by using the notion of Transfer Learning.</description>
    </item>
    
    <item>
      <title>Numeric representation of text documents: doc2vec how it works and how you implement it</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/04topicmodels/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/04topicmodels/</guid>
      <description>Numeric representation of text documents: doc2vec how it works and how you implement it Authors: Felix Idelberger, Alisa Kolesnikova, Jonathan Mühlenpfordt Introduction Natural language processing (NLP) received a lot of attention from academia and industry over the recent decade, benefiting from the introduction of new algorithms for processing the vast corpora of digitized text. A set of language modeling and feature learning techniques called word embeddings became increasingly popular for NLP tasks.</description>
    </item>
    
    <item>
      <title>Text Mining - Sentiment Analysis</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/05sentimentanalysis/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/05sentimentanalysis/</guid>
      <description>Sentiment Analysis using Deep Learning Authors: Katja Metzger, Aydin Sader Fosalaie, Ninos Yonan Outline   Sentiment Analysis Introduction    Case Study  Keras IMDB Dataset
 Data Analysis
 Word Embeddings
 Convolutional Neural Network
    Summary    Sentiment Analysis Introduction Sentiment analysis is a very beneficial approach to automate the classification of the polarity of a given text. A helpful indication to decide if the customers on amazon like a product or not is for example the star rating.</description>
    </item>
    
    <item>
      <title>Wide and Deep Learning Model for Grocery Product Recommendations</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/08recommendation/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/08recommendation/</guid>
      <description>Recommendation Systems Authors: Jingyi Liu, Brijesh Nanavati, Bharathi Srinivasan ## **Introduction** The explosion of information with the advent of the Internet and the multitude of choices available to customers introduces complexity in a customer’s decision processes. Recommender system is a useful information-filtering tool, which guides customers to a narrower selection of products and consequently helps them make better decisions. Matching users to the right products saves customers time and effort leading to increased user satisfaction, which in turn earns customer loyalty.</description>
    </item>
    
    <item>
      <title>A Manual on How To Write a Blog Post</title>
      <link>https://humboldt-wi.github.io/blog/research/instruction/manual/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/instruction/manual/</guid>
      <description>The Website of the Institute of Information Systems is based on a framework called Hugo. Hugo is a static website generator, which allows us to easily present your content of your deep learning projects. In order to present results in the best way, we combine this tool with another tool called Gist. Within this documentation you will learn how to work with those tools. The manual is composed of following sections:</description>
    </item>
    
    <item>
      <title>Image Captioning</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/07imagecaptioning/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/07imagecaptioning/</guid>
      <description>Image Captioning Authors: Severin Hußmann, Simon Remy, Murat Gökhan Yigit Introduction Image captioning aims for automatically generating a text that describes the present picture. In the last years it became a topic with growing interest in machine learning and the advances in this field lead to models that (depending on which evaluation) can score even higher than humans do. Image captioning can for instance help visually impaired people to grasp what is happening in a picture.</description>
    </item>
    
    <item>
      <title>Neural Network Fundamentals</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/01neuralnetworkfundamentals/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/01neuralnetworkfundamentals/</guid>
      <description>Neural Network Fundamentals Authors: Mahdi Bayat, Denis Augusto Pinto Maciel, Roman Proskalovich This blog post is a guide to help readers build a neural network from the very basics. It starts with an introduction to the concept of a neural networks concept and its early development. A step-by-step coding tutorial follows, through which relevant concepts are illustrated. Later in the post, there is also an introduction on how to build neural networks in Keras.</description>
    </item>
    
    <item>
      <title>Neural Networks into Production</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1718/09deeplearningintoproduction/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1718/09deeplearningintoproduction/</guid>
      <description>Neural Networks into Production Authors: Mahdi Bayat, Denis Augusto Pinto Maciel, Roman Proskalovich Motivation Training and tuning machine learning model is a hard task. There are many variables involved that can make or break your results. However, also very important is, after fine tuning your model, to be able to deploy it, so it can be accessed by other researchers, developers and applications.
A very common way to accomplish that is by using an API.</description>
    </item>
    
    <item>
      <title>Sample Post</title>
      <link>https://humboldt-wi.github.io/blog/research/instruction/00samplepost/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/instruction/00samplepost/</guid>
      <description>TEST #hugo
import numpy as np import pandas as pd 2+3 A Gentle Introduction to Neural Network Fundamentals  #### Imagine the following problem: There are handwritten numbers that you want computer to correctly clasify. It would be an easy task for a person but an extremely complicated one for a machine, especially, if you want to use some traditional prediction model, like linear regression. Even though the computer is faster than the human brain in numeric computations, the brain far outperforms the computer in some tasks.</description>
    </item>
    
    <item>
      <title>Analysis of social media behavior of the 2020 presidential election candidates</title>
      <link>https://humboldt-wi.github.io/blog/research/information_systems_1920/analysis_of_social_media_behavior_of_2020_presidential_elections_candidates_final/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://humboldt-wi.github.io/blog/research/information_systems_1920/analysis_of_social_media_behavior_of_2020_presidential_elections_candidates_final/</guid>
      <description>+++ title = &amp;ldquo;Analysis of social media behavior of the 2020 presidential election candidates&amp;rdquo; date = &amp;lsquo;2020-02-07&amp;rsquo; tags = [ &amp;ldquo;Fasttext&amp;rdquo;, &amp;ldquo;CNN&amp;rdquo;, &amp;ldquo;Class19/20&amp;rdquo;, &amp;ldquo;Sentiment Analysis&amp;rdquo;] categories = [&amp;ldquo;course projects&amp;rdquo;] banner = &amp;ldquo;img/seminar/sample/hu-logo.jpg&amp;rdquo; author = &amp;ldquo;Seminar Information Systems (WS19/20)&amp;rdquo; disqusShortname = &amp;ldquo;https-humbodt-wi-github-io-blog&amp;rdquo; description = &amp;ldquo;This blog post analyzes the tweets of the 2020 presidential candidates using Fasttext and CNN&amp;rdquo; +++
Authors: Georg Velev, Iliyana Pekova Table of Content  Introduction
 Data Retrieval</description>
    </item>
    
  </channel>
</rss>