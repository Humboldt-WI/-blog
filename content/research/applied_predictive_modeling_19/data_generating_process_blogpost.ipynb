{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+++\n",
    "title = \"Data Generating Process Simulation: The opossum package\"\n",
    "date = '2019-07-24'\n",
    "tags = [ \"Causal Inference\", \"Class19\", \"Data Simulation\"]\n",
    "categories = [\"Course projects\"]\n",
    "banner = \"img/seminar/data_generating_process/opossum.jpg\"\n",
    "author = \"Julian Winkel, Tobias Krebs\"\n",
    "disqusShortname = \"https-humbodt-wi-github-io-blog\"\n",
    "description = \"Generating datasets with observed treatment effects for model evaluation\"\n",
    "+++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generating Process Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topic\" data-toc-modified-id=\"Topic-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Topic</a></span></li><li><span><a href=\"#Motivation\" data-toc-modified-id=\"Motivation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Motivation</a></span></li><li><span><a href=\"#Properties\" data-toc-modified-id=\"Properties-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Properties</a></span></li></ul></li><li><span><a href=\"#Literature\" data-toc-modified-id=\"Literature-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Literature</a></span></li><li><span><a href=\"#Theory-behind-the-package\" data-toc-modified-id=\"Theory-behind-the-package-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Theory behind the package</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Model:-Partial-Linear-Regression\" data-toc-modified-id=\"General-Model:-Partial-Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>General Model: Partial Linear Regression</a></span></li><li><span><a href=\"#Covariates-generation\" data-toc-modified-id=\"Covariates-generation-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Covariates generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Continuous-covariates\" data-toc-modified-id=\"Continuous-covariates-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Continuous covariates</a></span></li><li><span><a href=\"#Binary-and-categorical-covariates\" data-toc-modified-id=\"Binary-and-categorical-covariates-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Binary and categorical covariates</a></span></li></ul></li><li><span><a href=\"#Treatment-assignment\" data-toc-modified-id=\"Treatment-assignment-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Treatment assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random\" data-toc-modified-id=\"Random-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Random</a></span></li><li><span><a href=\"#Dependent-on-covariates\" data-toc-modified-id=\"Dependent-on-covariates-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Dependent on covariates</a></span></li></ul></li><li><span><a href=\"#Treatment-effects\" data-toc-modified-id=\"Treatment-effects-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Treatment effects</a></span><ul class=\"toc-item\"><li><span><a href=\"#Positive-&amp;-negative-constant-effect\" data-toc-modified-id=\"Positive-&amp;-negative-constant-effect-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Positive &amp; negative constant effect</a></span></li><li><span><a href=\"#Positive-&amp;-negative-continuous-heterogeneous-effect\" data-toc-modified-id=\"Positive-&amp;-negative-continuous-heterogeneous-effect-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Positive &amp; negative continuous heterogeneous effect</a></span></li><li><span><a href=\"#No-effect\" data-toc-modified-id=\"No-effect-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>No effect</a></span></li><li><span><a href=\"#Discrete-heterogeneous-treatment-effect\" data-toc-modified-id=\"Discrete-heterogeneous-treatment-effect-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Discrete heterogeneous treatment effect</a></span></li></ul></li><li><span><a href=\"#Output-variable\" data-toc-modified-id=\"Output-variable-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Output variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Continuous\" data-toc-modified-id=\"Continuous-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Continuous</a></span></li><li><span><a href=\"#Binary\" data-toc-modified-id=\"Binary-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Binary</a></span></li></ul></li></ul></li><li><span><a href=\"#Package-application\" data-toc-modified-id=\"Package-application-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Package application</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choosing-covariates\" data-toc-modified-id=\"Choosing-covariates-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Choosing covariates</a></span></li><li><span><a href=\"#Creating-treatment-effects\" data-toc-modified-id=\"Creating-treatment-effects-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Creating treatment effects</a></span></li><li><span><a href=\"#Creating-output\" data-toc-modified-id=\"Creating-output-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Creating output</a></span></li><li><span><a href=\"#Other-functions\" data-toc-modified-id=\"Other-functions-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Other functions</a></span></li></ul></li><li><span><a href=\"#Example:-Applying-double-machine-learning\" data-toc-modified-id=\"Example:-Applying-double-machine-learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Example: Applying double machine learning</a></span></li><li><span><a href=\"#Discussion\" data-toc-modified-id=\"Discussion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Discussion</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Topic\n",
    "As modern science becomes increasingly data-driven among virtually all fields, it is obligatory to inspect not only how scientists analyze data but also *what kind* of data is used. Naturally, the performance of a model is bound by the quality of underlying data.\n",
    "This blog post explores the properties and constituents of realistic data sets and proposes flexible and user-friendly software to support research by means of 'Simulated Data Generating Processes' (SDGP).\n",
    "\n",
    "### Motivation\n",
    "With increasing dimension, standard Machine Learning Techniques tend to suffer from the 'Curse of Dimensionality', referring to the phenomenon of data points becoming sparse for constant sample sizes, as well as large parameter spaces which render consistent parameter estimation to be difficult (Chernozhukov).\n",
    "However, having more and more data available, these problems need to be addressed in a specialized framework.\n",
    "Another fundamental problem that researchers face is the non-observability of counterfactuals and treatment effects, firstly addressed in the 'Potential Outcomes Framework' (Neyman, Rubin). \n",
    "### Properties \n",
    "The researcher's setting is located in a high-dimensional, partial-linear model.\n",
    "The covariates are pseudo-randomly generated in such a manner that they are (partially) correlated with the output variable, which itself can take binary, discrete or continuous values. The relation between the covariates and the output variable can be specified as linear, non-linear,(heterogeneous or a mixture // move to treatment effect).\n",
    "Individual treatment assignment, propensity, being the probability of being in the treatment group, can be random or non-random. One of the fundamental problems researchers face is the non-observability of counterfactuals and treatment effects, firstly addressed in the 'Potential Outcomes Framework' (Neyman, Rubin). In the proposed setting, treatment effects are *known* and can be customized in their effect.\n",
    "By proposing a model in which all components are expounded, researchers receive support to evaluate and compare various models that are applied to a realistic data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "Chernozhukov et al. address some problems in the area of modern, prediction focussed machine learning.\n",
    "Using Robinson's framework, they argue, standard techniques to estimate statistics as 'regression coefficients,\n",
    "average treatment effects, average lifts, and demand or supply elasticities' can easily fail\n",
    "due to regularization bias. They suggests to estimate and compensate the introduced\n",
    "bias in a double machine learning setting in order to restrict estimated parameters\n",
    " to a reasonably small space. The resulting estimators are proven to have desirable statistical properties;\n",
    " hence presenting a method to apply common machine learning methods as 'random forest, lasso, ridge, deep neural nets, boosted trees' in a highdimensional and nonlinear model.\n",
    "\n",
    "'some methods for hetereogeneous treatment effect estimation in high-dimensions'\n",
    "Within such a framework, Powers et al. provide an overview to estimate heterogeneous\n",
    "treatment effects for medical applications across different models.  \n",
    "// extend this\n",
    "\n",
    "'Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition'\n",
    "Dorie et al. compare inferential strategies from a data analysis competition.\n",
    "The authors criticize a general lack of guidance on optimal applications of statistical models.\n",
    "They discuss the problem of adequately assigning participants to test and control groups,\n",
    "where unobserved characteristics require a tradeoff between a demanding parametric structure or flexibility.\n",
    "Results from a competition for various models and strategies are reported and compared, yielding in an overall surplus for flexibility.\n",
    "Moreover, they provide a list of un-addressed issues to be considered for application,\n",
    "including but not limited to non-binary treatment, non-continuous response,\n",
    "non-iid data, varying data / covariate size. Our proposed model accounts for the mentioned issues.\n",
    "\n",
    "Software Review\n",
    "\n",
    "Naturally, software packages that generate synthetic data sets already exist.\n",
    "Given that the most popular languages for data analysis are R and Python, it\n",
    "is desirable to provide the according software in the respective environment.\n",
    "One existing R Package to deal with data generating processes is `simstudy`.\n",
    "\n",
    "While `simstudy` provides great flexibility, using low-level functions to generate desirable relationships,\n",
    "it comes at the cost of properly understanding their backend and the ability to create\n",
    "a sound statistical model with regard to the probability theoretic background.\n",
    "Furthermore, it is costly to create high dimensional data sets.\n",
    "\n",
    "To enrich and broaden the state of the art, our proposed model aims to strip\n",
    "the user of this burden by offering a backend which can be easily accessed,\n",
    "but by no means has to be regarded at all by the user.\n",
    "Hence the informed user can choose himself how deeply he wants to explore the underlying\n",
    "mathematics.\n",
    "On top of the backend, we offer an easily-adjustable user interface for common\n",
    "scientific approaches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory behind the package\n",
    "\n",
    "In the following sections you will find a step by step explanation of how our data simulation package works internally, accompanied by the corresponding formulas, code snippets and explanatory graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model: Partial Linear Regression\n",
    "\n",
    "The model that our package is based on is a partial linear regression model as it is described in Chernozhukov et al. (2016). It consists of the covariates X that are possibly non-linear in relation to y, the treatment term consisting the treatment effect and the treatment assginment vector and an normally distributed error term. \n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ \\begin{align} Y = \\theta_0  D + g_0(X)+ U,  &&  & E[U|X,D] = 0 \\\\ \n",
    "   D = m_0(X) + V, &&   &  E[V|X] = 0 \\\\\n",
    "  \\theta_0 =  t_0(Z) + W, && & E[W|Z] = 0, \\; Z \\subseteq X  \\\\\n",
    "   \\end{align}$$\n",
    "\n",
    "$Y$ - Outcome Variable $\\quad \\theta_0$ - True treatment effect $\\quad D$ - Treatment Dummy $ \\quad X_{n*k}$ - Covariates\n",
    "\n",
    "$U$, $V$ & $W$ - normally distributed error terms with expected value 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariates generation\n",
    "\n",
    "#### Continuous covariates\n",
    "\n",
    "The covariate matrix $X$ in our simulation is drawn from a multivariate normal distribution with an expected value of 0 and a specified covariance matrix $\\Sigma$. $\\Sigma$ is constructed the following way. First, values for Matrix $A$ are drawn from a uniform distribution. In a second step, to make sure that there exist negative correlations and that not all variables are highly correlated with each other, we create an overlay matrix $B$. This overlay matrix $B$ consists of values 1 and -1. Third, we multiply the two matrices element-wise and adjust the result with a correction term to assure that values in $\\Sigma$ are not increasing in $k$. This result is represented by the matrix $\\Lambda$. In a final step, we calculate $\\Sigma$ by multiplying $\\Lambda$ with its transposed to assure that it is positive definite. \n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ X_{n*k} \\sim N_k(0,\\Sigma)$$\n",
    "\n",
    "Where, \n",
    "\n",
    "$n = Number \\; of \\; Observations, \\quad k =  Number \\; of \\; Covariates$\n",
    "\n",
    "$\\Sigma = \\Lambda*\\Lambda^T, \\quad \\Lambda = \\frac{10}{k} (A \\circ B), \\quad A \\sim U(0,1), \\quad B \\sim Bernoulli(0.5)\\;,B \\in \\{-1,1\\}$\n",
    "\n",
    "$Matrices \\; A, \\; B, \\; and \\; \\Lambda \\; are \\; all \\; of \\; dimension \\; k*k$\n",
    "\n",
    "<br/>\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/f550c942f3ceea379271c9d89913fac7.js\"></script>\n",
    "\n",
    "The following heat-map shows an example of Sigma with k=10 covariates. Depending on the chosen random seed, typically correlations range between -0.7 and 0.7 with slightly varying minimum and maximum values.\n",
    "\n",
    "<img align=\"center\" width=\"660\" height=\"500\" style=\"display:block;margin:0 auto;\" src=\"/blog/img/seminar/data_generating_process/covariates_correlation.png\">\n",
    "\n",
    "#### Binary and categorical covariates\n",
    "Binary and categorical covariates are created by transforming the continuous covariates $X$. Depending on what option is chosen, the package simulates all covariates or a subset of $X$ as binary and/or categorical. The covariates are transformed by min-max-standardization to obtain prediction values. These values are then used to either obtain binary realizations from a Bernoulli distribution or categorical covariates by summing up the realizations of several Bernoulli draws.\n",
    "\n",
    "$$ p_{j} = \\frac{x(j) - min(x(j))}{max(x(j)) - min(x(j))} $$\n",
    "\n",
    "$$ x_{binary}(j) = Bernoulli\\left( p_{j} \\right) $$\n",
    "\n",
    "\n",
    "$$ x_{categorical}(j) = \\sum_{c=1}^{C-1} Bernoulli_c \\left( p_{j} \\right) $$\n",
    "\n",
    "$x(j) \\in X = covariate\\; vector, \\quad C = number\\; of\\; categories$\n",
    "\n",
    "<br/>\n",
    "\n",
    "The following code serves as an example of how binary and categorical covariates are implemented. In this example, the user chooses how many covariates are transformed and gives a list of categories that should be included, e.g. [2,5] for binary and categorical variables with 5 categories. \n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/c9d325e584421595ed1b012fe090d6fc.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment assignment\n",
    "There are two ways treatment can be assigned. The first one is to simulate a random control trial where all observations are assigned treatment with the same probability $m_0$. The second one is assigning treatment in the fashion of an observational study where assignment of treatment depends on covariates $X$ and probability $m_0$ differs between observations. Either way, the assignment vector $D$ is drawn from a Bernoulli distribution with probability $m_0$.\n",
    "\n",
    "$$ D_{n*1} \\sim Bernoulli(m_0) $$\n",
    "\n",
    "#### Random \n",
    "In the random assignment case, only an assignment probability $m_0$ has to be chosen which is then used to draw realizations from a Bernoulli distribution to create the assignment vector $D$.\n",
    "\n",
    "$$ m_0 \\in [0,1] $$\n",
    "\n",
    "#### Dependent on covariates\n",
    "To obtain the individual probabilities, as a first step, a subset $Z \\subseteq X$  with dimensions $n$ times $l$ is chosen. Then, $Z$ is multiplied with a weight vector $b$, which is made up of values drawn from a uniform distribution, resulting in vector $a$. The standardized version of vector $a$ is then used to draw values from a Normal CDF which eventually serve as assignment probabilities in vector $m_0$. To include some randomness, random noise, drawn from a uniform distribution and labeled as $\\eta$, is added to $a$ before the standardization. \n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ m_{0, n*1} = \\Phi\\left(\\frac{a-\\hat{\\mu}(a)}{\\hat{\\sigma}(a)}\\right) $$\n",
    "\n",
    "Where,\n",
    "\n",
    "$ a_{n*1} = Z * b + \\eta, \\quad Z_{n*l} \\subseteq X_{n*k}, \\quad b_{l*1} \\sim U(0,1), \\quad  \\eta_{n*1} \\sim U(0,0.25)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/4b94f744158ec4ee2411075c2cf66e06.js\"></script>\n",
    "\n",
    "The following histograms show the distributions of propensity scores ($m_0$) in the case of non-random assignment with low, medium, and high average assignment probability. In the trivial case of random assignment, propensity scores are all the same for each observation. \n",
    "\n",
    "<img align=\"center\" width=\"800\" height=\"350\" style=\"display:block;margin:0 auto;\" src=\"/blog/img/seminar/data_generating_process/propensity_score_plot.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment effects\n",
    "The package offers 6 different options for the treatment effect $\\theta_0\n",
    "$. These effects are: Positive constant, negative constant , positive heterogeneous, negative heterogeneous, no effect, and discrete heterogeneous. When applying these treatment effects, one can choose a single option or pick preferred options and apply a mix of them. \n",
    "\n",
    "#### Positive & negative constant effect\n",
    "In the case of a constant effect, treatment is the same for each individual and, thus, is just a constant $c$. Depending on the chosen sign either positive or negative.\n",
    "\n",
    "**Option 1** positive constant: $$ \\theta_0 = c $$\n",
    "\n",
    "**Option 2** negative constant: $$\\theta_0 = - \\; c$$\n",
    "\n",
    "#### Positive & negative continuous heterogeneous effect\n",
    "In contrast to the constant effect, for which each observation has the same treatment effect, the continuous heterogeneous effect differs between individuals within a specified interval. Moreover, depends the size of the individual treatment effect on a subset Z of the covariates X. The creation of this treatment effect begins similar to the treatment assignment with taking the dot product of the subset $Z$ and the same weight vector used in the treatment assignment $b$. The result is put into a sinus function and added an normally distributed error term $W$. The result $\\gamma$ is then min-max-standardized and adjusted in size with a constant $c$. Note that eventually the size of treatment depends on the intensity chosen which will be explained later on in the application part. In case that the heterogeneous effect is supposed to be partly or entirely negative, the resulting distribution is shifted by the respective quantile value $q_{neg}$ that corresponds to the wanted negative percentage share. \n",
    "\n",
    "<br/>\n",
    "\n",
    "$$\\gamma =  sin(Z * b) + W$$\n",
    "\n",
    "\n",
    "$$\\theta_0 = \\frac{\\gamma - min(\\gamma)}{max(\\gamma) - min(\\gamma)}*c - q_{neg}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$Z \\subseteq X,\\quad  b_{k*1} \\stackrel{ind.}{\\sim} U(0,1), \\quad W \\stackrel{ind.}{\\sim} N(0,1), \\quad q_{neg} \\in [0,c], \\quad c = size \\; adjustment \\; parameter$\n",
    "\n",
    "<br/>\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/80c8db186625da23d99caebf1bbfc913.js\"></script>\n",
    "\n",
    "The following histogram shows an example where 30% of the continuous heterogeneous treatment effect is negative and 70% is positive. \n",
    "\n",
    "<img align=\"center\" width=\"500\" height=\"400\" style=\"display:block;margin:0 auto;\" src=\"/blog/img/seminar/data_generating_process/pos_neg_heterogeneous_treatment_effect.png\">\n",
    "\n",
    "\n",
    "#### No effect\n",
    "This option allows that there are cases of treatment assignment that show no effect, i.e. $\\theta_0$ is simply 0.\n",
    "\n",
    "$$\\theta_0 = 0$$\n",
    "\n",
    "\n",
    "#### Discrete heterogeneous treatment effect\n",
    "The discrete heterogeneous treatment effect is created in a similar manner as the continuous version. The scalar product of a subset of the covariates is put into a sinus function and the result is min-max-standardized to serve as probability. In another step, those probabilities are used to draw values from a Bernoulli distribution, where 0's are exchanged with the lower value and 1's with the higher value of the new treatment effect. The resulting treatment effect depends then on at least a subset of the covariates. \n",
    "\n",
    "$$\\gamma =  sin(Z * b)$$\n",
    "\n",
    "$$ p_{dh} = \\frac{\\gamma - min(\\gamma)}{max(\\gamma) - min(\\gamma)}$$\n",
    "\n",
    "$$ \\theta_0 \\sim Bernoulli(p_{dh}), \\; \\theta_0 \\in \\{0.1*c , 0.2*c\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output variable\n",
    "#### Continuous\n",
    "The continuous output variable $Y$ comprises of three main parts. The treatment part $\\theta_0 D $ as explained above, the possibly non-linearly transformed covariates $X$, and the error term $U$. The transformation $g_0()$ consists of two parts. First, the scalar product of $X$ and the weighting vector $b$ is taken. The values of $b$ are drawn from an uneven beta distribution which assures that a few covariates impact the outcome much more than others which seems more realistic than e.g. a uniformly distributed impact. Second, there is either no further transformation, i.e. the relation is linear, a partial non-linear transformation which consists of a linear and non-linear part, or an entirely non-linear transformation. Moreover, the package offers an option to add interaction terms of the form $x_i*x_j$ that are drawn randomly out of $X$ and added into $g_0()$. The number of interaction terms $I$ is set to $\\sqrt{k}$ as a default and can be adjusted if wanted. \n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ Y = \\theta_0  D + g_0(X)+ U, \\quad E[U|X,D] = 0$$\n",
    "\n",
    "where,\n",
    "\n",
    "$g_0(x) \\in  \\{x, \\; \\; 2.5*cos(x)^3 + 0.5*x, \\; \\; 3*cos(x)^3\\}, \\qquad x \\in \\{X_{n*k}*b_{k*1}, \\quad X_{n*k}*b_{k*1} + X_{in,n*I}*b_{in,I*1} \\},$\n",
    "\n",
    "$b_{k*1} \\sim Beta(1,5), \\qquad X_{in}*b_{in} = \\sum_{i,j=1}^{I} b_{i,1*1} * (x_{i,n*1} \\circ x_{j,n*1}), \\qquad i,j \\in \\{i_1,...,i_I\\} \\sim U\\{0,k\\}, $\n",
    "\n",
    "$I = number \\; of \\; interaction \\; terms$\n",
    "\n",
    "The following code snippet shows a slightly simplified version of how the continuous output variable is implemented with the two examples of a simple linear transformation and a non-linear transformation including interaction terms.\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/c8a7a18db3149d5762070fca1315db8e.js\"></script>\n",
    "\n",
    "\n",
    "The following scatter plots display the three different types of relation between X and y that can be chosen. The underlying data was simulated without treatment effects or interaction terms. \n",
    "\n",
    "<img align=\"center\" width=\"850\" height=\"350\" style=\"display:block;margin:0 auto;\" src=\"/blog/img/seminar/data_generating_process/y_transformations_plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary\n",
    "To simulate a binary outcome, the continuous outcome is transformed and then used as probabilities to draw $Y$ from a Bernoulli distribution. $Y_{binary}$ consists of the possibly non-linearly transformed covariates $g_0(X)$ that are min-max-standardized and the treatment vector $\\theta_0$ that is simply adjusted by dividing all values by 10. The range of the standardized values is set to $[0.1,0.9]$ to assure that treatment effects always have influence on the outcome probability.  \n",
    "\n",
    "\n",
    "$$ p_{n*1} = \\frac{\\delta - min(\\delta)}{max(\\delta) - min(\\delta)}*0.8 + 0.1 + \\theta_{binary}*D$$\n",
    "\n",
    "$$Y_{n*1} \\sim Bernoulli(p)$$\n",
    "\n",
    "where,\n",
    "\n",
    "$p \\in [0,1], \\quad \\theta_{binary} = \\frac{1}{10} * \\theta_0, \\quad \\delta = g_0(X) $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package application\n",
    "\n",
    "The package is applied by simply importing the `UserInterface`, initializing it, and using the two main methods `generate_treatment` and `output_data` as shown in the code below. The only required parameters the user has to choose are the number of observations N and the number of covariates k. All other parameters have default values that can be changed if needed. The following code gives all necessary lines to create a dataset and has all default values listed.\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/72319f8e5b96190d1fdadbe1a1cedd59.js\"></script>\n",
    "\n",
    "### Choosing covariates \n",
    "\n",
    "As a default, covariates are continuous. However, one can change that with the `categorical_covariates` parameter. It allows to make either all or a chosen number of covariates binary and/or categorical. Depending on how covariates should be transformed there are three options. Option one is to simply give an integer $i$ which then transforms the whole dataset into variables with i categories, e.g. $i$ = 2 makes all covariates binary. Option two is a list with two integers $[z,i]$, where the first one $z$ is the number of variables that are transformed, and the second one $i$ is again the number of categories, e.g. `[5,2]` generates 5 of all k covariates as binary variables. Option three is again a list but this time contains an integer and a list $[z,[i_1,..i_C]]$. As before, the first integer $z$ gives the number of covariates that are transformed, whereas the integers in the list $[i_1,...i_C]$ give the different frequencies of categories that are then equally common among the transformed covariates. As an example, `[10,[2,3,5]]` would lead to 10 transformed covariates of which four would be binary and three would have 3 and 5 categories respectively. \n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/02d7c3b424f19a894fcbab69775247e8.js\"></script>\n",
    "\n",
    "### Creating treatment effects\n",
    "The `generate_treatment()` function has two main purposes. Assigning treatment and generating the chosen treatment effects. As a default treatment is assigned randomly as in a random control trial, i.e. the `random_assignment` boolean parameter is set to `True`. Setting it to `False` makes treatment depending on a subset of covariates, simulating an observational study. The size of this subset is set to k/2 by default but can be adjusted with the function `set_subset_z_size_assignment()` that is further explained under the point Other functions below. The other parameter concerning assignment is `assignment_prob` which affects the average probability of treatment assignment, determining how many observations end up being treated. In the case of random assignment, parameter values can be freely chosen from the interval (0,1), however, when assignment is not random one has to choose between the three levels `'low'`, `'medium'`, and `'high'` which represent average probability values of 0.35, 0.5, and 0.65 respectively. \n",
    "\n",
    "Regarding the creation of treatment effects there are two ways on how to choose which kind of effects should be simulated. Either by setting the wanted parameters to `True` and unwanted to `False`, where the default is `constant_pos=True` and all other options False, or by passing a list of length 6 to `treatment_option_weights` containing the wanted percentages of treatment effect options that sum up to 1. If the first way, using booleans, is chosen then the frequency of each selected option is the same. This option is the easiest way to use when just one kind of effect is wanted. The second option, passing a list, offers a better way of customizing the wanted mix of treatment effects. Each value in the list corresponds to a kind of treatment effect the following way `[constant_pos, constant_neg, heterogeneous_pos, heterogeneous_neg, no_treatment, discrete_heterogeneous]`, e.g. `[0, 0, 0.8, 0.2, 0, 0]` generates 20% negative heterogeneous treatment and 80% positive heterogeneous treatment. \n",
    "\n",
    "The following code shows the application of the above discussed parameters. All parameters that are not shown are automatically set to their default values. \n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/c07617c63231c9e0a76886ac49d273a9.js\"></script>\n",
    "\n",
    "\n",
    "The intensity parameter adjusts the size of treatment effects. It takes floats of the interval [1,10] which determine the magnitude of the treatment effects the following way: \n",
    "- $constant: \\; intensity*0.2$, \n",
    "- $heterogeneous: \\; [0, intensity*0.4]$, \n",
    "- $discrete\\_heterogeneous: \\{intensity*0.1, intensity*0.2\\}$\n",
    "\n",
    "The interactive plot bellow shows different resulting magnitudes of the treatment effect as part of the outcome variable y in form of a kernal density estimation. Change the intensity value to see how it affects the outcome distributions of treated and not treated observations! \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  <div class=\"\n",
    "      cell border-box-sizing code_cell rendered\">\n",
    "    <div class=\"hidden\">\n",
    "\n",
    "<div class=\"inner_cell\">\n",
    "    <div class=\"input_area\">\n",
    "<div class=\" highlight hl-ipython3\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">ipywidgets</span> <span class=\"k\">as</span> <span class=\"nn\">widgets</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">ipywidgets</span> <span class=\"k\">import</span> <span class=\"n\">interact</span><span class=\"p\">,</span> <span class=\"n\">interactive</span><span class=\"p\">,</span> <span class=\"n\">fixed</span><span class=\"p\">,</span> <span class=\"n\">interact_manual</span>\n",
    "\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">interactive_plots_functions</span> <span class=\"k\">import</span> <span class=\"n\">output_difference_plt</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">opossum</span> <span class=\"k\">import</span> <span class=\"n\">UserInterface</span>\n",
    "\n",
    "\n",
    "<span class=\"k\">def</span> <span class=\"nf\">output_difference_interaction_fct</span><span class=\"p\">(</span><span class=\"n\">intensity</span><span class=\"p\">):</span>\n",
    "    <span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">UserInterface</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"n\">categorical_covariates</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">generate_treatment</span><span class=\"p\">(</span><span class=\"n\">intensity</span> <span class=\"o\">=</span> <span class=\"n\">intensity</span><span class=\"p\">)</span>\n",
    "\n",
    "    <span class=\"n\">y_continuous</span><span class=\"p\">,</span> <span class=\"n\">X_continuous</span><span class=\"p\">,</span> <span class=\"n\">assignment_continuous</span><span class=\"p\">,</span> <span class=\"n\">treatment_continuous</span> <span class=\"o\">=</span> <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">output_data</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">y_binary</span><span class=\"p\">,</span> <span class=\"n\">X_binary</span><span class=\"p\">,</span> <span class=\"n\">assignment_binary</span><span class=\"p\">,</span> <span class=\"n\">treatment_binary</span> <span class=\"o\">=</span> <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">output_data</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
    "\n",
    "\n",
    "\n",
    "    <span class=\"n\">y_treated_continuous</span> <span class=\"o\">=</span> <span class=\"n\">y_continuous</span><span class=\"p\">[</span><span class=\"n\">assignment_continuous</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
    "    <span class=\"n\">y_not_treated_continuous</span> <span class=\"o\">=</span> <span class=\"n\">y_continuous</span><span class=\"p\">[</span><span class=\"n\">assignment_continuous</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
    "    <span class=\"n\">y_treated_binary</span> <span class=\"o\">=</span> <span class=\"n\">y_binary</span><span class=\"p\">[</span><span class=\"n\">assignment_binary</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
    "    <span class=\"n\">y_not_treated_binary</span> <span class=\"o\">=</span> <span class=\"n\">y_binary</span><span class=\"p\">[</span><span class=\"n\">assignment_binary</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n",
    "\n",
    "    \n",
    "    <span class=\"n\">output_difference_plt</span><span class=\"p\">(</span><span class=\"n\">y_treated_continuous</span><span class=\"p\">,</span> <span class=\"n\">y_not_treated_continuous</span><span class=\"p\">,</span> \n",
    "                          <span class=\"n\">y_treated_binary</span><span class=\"p\">,</span> <span class=\"n\">y_not_treated_binary</span><span class=\"p\">)</span>\n",
    "    \n",
    "\n",
    "\n",
    "<span class=\"n\">interactive</span><span class=\"p\">(</span><span class=\"n\">output_difference_interaction_fct</span><span class=\"p\">,</span> <span class=\"n\">intensity</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n",
    "</pre></div>\n",
    "\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"output_wrapper\">\n",
    "<div class=\"output\">\n",
    "\n",
    "\n",
    "<div class=\"output_area\">\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  <div class=\"output_subarea output_widget_view \">\n",
    "    <button class=\"js-nbinteract-widget\">\n",
    "      Loading widgets...\n",
    "    </button>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "  </div>\n",
    "\n",
    "\n",
    "\n",
    "<!-- Loads nbinteract package -->\n",
    "<script src=\"https://unpkg.com/nbinteract-core\" async></script>\n",
    "<script>\n",
    "  (function setupNbinteract() {\n",
    "    // If NbInteract hasn't loaded, wait one second and try again\n",
    "    if (window.NbInteract === undefined) {\n",
    "      setTimeout(setupNbinteract, 1000)\n",
    "      return\n",
    "    }\n",
    "\n",
    "    var interact = new window.NbInteract({\n",
    "      spec: 'Tobias-K93/5d6f9a8b7de571a2248533ef327c6f8f/master',\n",
    "      baseUrl: 'https://mybinder.org',\n",
    "      provider: 'gist',\n",
    "    })\n",
    "    interact.prepare()\n",
    "\n",
    "    window.interact = interact\n",
    "  })()\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating output\n",
    "The second main functions is `output_data()` which internally creates the output variable y and returns a tuple including the 4 numpy arrays of interest. In the sample code below denoted as `y` is the array of size N with the output variable values, `X` is an array of size N*k consisting of the simulated covariates, `assignment` is an array of length N consisting of zeros and ones, and `treatment` is an array of length N consisting of the realized treatment effect values. \n",
    "\n",
    "The function provides two parameters that can be adjusted. `binary` simply allows to change the output variable y from continuous to binary while the default is `False` which makes y continuous. `x_y_relation` defines the relationship of y and X that is used offering 6 different options one can choose: `'linear_simple'`, `'linear_interaction'`, `'partial_nonlinear_simple'`, `'partial_nonlinear_interaction'`, `'nonlinear_simple'`, `'nonlinear_interaction'`. For more information about the implications of each option see part *3.5 Output variable*.\n",
    "\n",
    "<script src=\"https://gist.github.com/Tobias-K93/a9e602687253fa3c3e60810b9810ffa4.js\"></script>\n",
    "\n",
    "In the drop down menu below you can choose between the different options of `x_y_relation` and get a scatter plot of the combined X and, if the interaction version is chosen, interaction terms of X plotted on the y values. The underlying dataset is created with N=10000 observations and k=50 covariates. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "\n",
    "  <div class=\"\n",
    "      cell border-box-sizing code_cell rendered\">\n",
    "    <div class=\"hidden\">\n",
    "\n",
    "<div class=\"inner_cell\">\n",
    "    <div class=\"input_area\">\n",
    "<div class=\" highlight hl-ipython3\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">ipywidgets</span> <span class=\"k\">import</span> <span class=\"n\">interactive</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">interactive_plots_functions</span> <span class=\"k\">import</span> <span class=\"n\">x_y_relation_plot</span>\n",
    "<span class=\"kn\">from</span> <span class=\"nn\">opossum</span> <span class=\"k\">import</span> <span class=\"n\">UserInterface</span>\n",
    "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
    "\n",
    "<span class=\"k\">def</span> <span class=\"nf\">scatter_output_fct</span><span class=\"p\">(</span><span class=\"n\">x_y_relation</span><span class=\"p\">):</span>\n",
    "    <span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">UserInterface</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">,</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"n\">categorical_covariates</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">)</span>\n",
    "    <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">generate_treatment</span><span class=\"p\">()</span>\n",
    "    <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">assignment</span><span class=\"p\">,</span> <span class=\"n\">treatment</span> <span class=\"o\">=</span> <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">output_data</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">x_y_relation</span> <span class=\"o\">=</span> <span class=\"n\">x_y_relation</span><span class=\"p\">)</span>\n",
    "    \n",
    "    <span class=\"k\">if</span>  <span class=\"s1\">&#39;interaction&#39;</span> <span class=\"ow\">in</span> <span class=\"n\">x_y_relation</span><span class=\"p\">:</span>\n",
    "        <span class=\"n\">g_0_X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">get_weigths_covariates_to_outputs</span><span class=\"p\">())</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">backend</span><span class=\"o\">.</span><span class=\"n\">X_interaction</span><span class=\"p\">,</span> <span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">backend</span><span class=\"o\">.</span><span class=\"n\">weights_interaction</span><span class=\"p\">)</span>\n",
    "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
    "        <span class=\"n\">g_0_X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">u</span><span class=\"o\">.</span><span class=\"n\">get_weigths_covariates_to_outputs</span><span class=\"p\">())</span>\n",
    "\n",
    "    <span class=\"n\">x_y_relation_plot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">g_0_X</span><span class=\"p\">)</span>\n",
    "    \n",
    "<span class=\"n\">sco_plot</span> <span class=\"o\">=</span> <span class=\"n\">interactive</span><span class=\"p\">(</span><span class=\"n\">scatter_output_fct</span><span class=\"p\">,</span> <span class=\"n\">x_y_relation</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;linear_simple&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;linear_interaction&#39;</span><span class=\"p\">,</span>\n",
    "                                                           <span class=\"s1\">&#39;partial_nonlinear_simple&#39;</span><span class=\"p\">,</span> \n",
    "                                                           <span class=\"s1\">&#39;partial_nonlinear_interaction&#39;</span><span class=\"p\">,</span> \n",
    "                                                           <span class=\"s1\">&#39;nonlinear_simple&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nonlinear_interaction&#39;</span><span class=\"p\">])</span>\n",
    "<span class=\"n\">sco_plot</span>\n",
    "</pre></div>\n",
    "\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"output_wrapper\">\n",
    "<div class=\"output\">\n",
    "\n",
    "\n",
    "<div class=\"output_area\">\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  <div class=\"output_subarea output_widget_view \">\n",
    "    <button class=\"js-nbinteract-widget\">\n",
    "      Loading widgets...\n",
    "    </button>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "While the two main functions are enough to produce a dataset, it can be necessary to make use of other functions to adjust some specific attributes or use other features of the package. Most of the functions' names should be self-explanatory and under the `help()` command there is a documentation for each function in case it is not clear how it should be used or what its purpose is. The functions are:\n",
    "\n",
    "`plot_covariates_correlation()`\n",
    "\n",
    "`get_propensity_scores()`\n",
    "    \n",
    "`get_weights_treatment_assignment()`\n",
    "    \n",
    "`get_weights_covariates_to_outputs()`\n",
    "    \n",
    "`get_treatment_effect_type()`\n",
    "\n",
    "`set_weights_treatment_assignment(new_weight_vector)`\n",
    "            \n",
    "`set_weights_covariates_to_outputs(new_weight_vector)`\n",
    "\n",
    "`set_subset_z_size_assignment(new_size)`\n",
    "\n",
    "`set_subset_z_size_treatment`\n",
    "\n",
    "`set_interaction_num(new_num)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Applying double machine learning \n",
    "\n",
    "In the following we present a show case on how to use our proposed data generator.\n",
    "As mentioned before, the double machine learning framework of Chernozhukov et al., applied on Robinson's partial\n",
    "linear model, deals with the problem of biased parameter estimates (e.g. treatment effect)\n",
    "due to high dimensionality.\n",
    "(for detailed info link the double ml blog post)\n",
    "Hence we demonstrate the performance of three different parameter estimators.\n",
    "The employed models are an Ordinary Least Squares Regression,\n",
    "a naive Double-ML Random Forest and another Double-ML model in a cross-fitted version.\n",
    "\n",
    "By design the true parameter Theta is known to be 1. Densities are reported as follows:\n",
    "\n",
    "<img align=\"center\" width=\"850\" height=\"350\" style=\"display:block;margin:0 auto;\" src=\"/blog/img/seminar/data_generating_process/dml_estimator_distribution.png\">\n",
    "\n",
    "Where the first plot depicts the result of randomly assigned treatment and the second\n",
    "one uses non-random assignment. While the OLS estimator has a strong bias due to\n",
    "the non-linearity of the underlying dataset, one can clearly see the difference\n",
    "in the accuracy of the Double-ML models.\n",
    "In case of random assignment, cross-fitted Double-ML as well as the naive approach\n",
    "correctly identify the 'unknown' parameter. However, when switching to non-random assignment,\n",
    "the performance of the naive model quickly deteriorates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
